{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import davos\n",
    "except:\n",
    "    %pip install davos\n",
    "    import davos\n",
    "davos.config.suppress_stdout = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jmanning/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "smuggle requests                                            # pip: requests==2.28.2\n",
    "from tqdm smuggle tqdm                                      # pip: tqdm==4.65.0\n",
    "smuggle pandas as pd                                        # pip: pandas==2.0.1\n",
    "smuggle numpy as np                                         # pip: numpy==1.25.2\n",
    "smuggle seaborn as sns                                      # pip: seaborn==0.12.2\n",
    "from matplotlib smuggle pyplot as plt                       # pip: matplotlib==3.7.1\n",
    "from IPython.display smuggle Markdown\n",
    "smuggle openpyxl                                           # pip: openpyxl==3.1.2\n",
    "\n",
    "from nltk.tokenize smuggle word_tokenize, sent_tokenize     # pip: nltk==3.8.1\n",
    "from nltk smuggle pos_tag\n",
    "\n",
    "smuggle nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "smuggle contractions                                       # pip: contractions==0.1.73\n",
    "\n",
    "smuggle re\n",
    "smuggle os\n",
    "smuggle urllib\n",
    "smuggle json\n",
    "smuggle string\n",
    "smuggle warnings\n",
    "smuggle pickle\n",
    "from glob smuggle glob as lsdir\n",
    "from collections import defaultdict\n",
    "\n",
    "from pathlib smuggle Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting references to past and future events\n",
    "\n",
    "At a high level, the goal of this meta analysis is to predict in-text references to past and future events.  Manually identifying these references is labor and time intensive, so it is impractical to scale up manual tagging to millions of documents.  Instead, we've defined some heuristics for *predicting* when text is referring to real or hypothetical past or future events.  Our approach comprises four main steps:\n",
    "\n",
    "1. First we use the `nltk` package to segment each document into individual sentences. Each sentence is processed independently of the others.\n",
    "2. Next, we handle contractions using the `contractions` package (e.g., \"we'll\" gets split into \"we will,\" and so on).\n",
    "3. Third, we define two sets of \"keywords\" (words and phrases) that tend to be indicative of referring to the past (`past_keywords.txt`) or future (`future_keywords.txt`).  We used ChatGPT (`gpt-4`) to generate each list, with exactly 50 templates per list, using the following prompt:\n",
    "```\n",
    "I'm designing a heuristic algorithm for identifying references (in text) to past and future events. Part of the algorithm will involve looking for specific keywords or phrases that suggest that the text is referring to something that happened (or will happen) in the past and/or future. Could you help me generate a list of 50 keywords or phrases to include in each list (one list for identifying references to the past and a second list for identifying references to the future)? I'd like to be able to paste the lists you generate into two plain text documents with one row per keyword or phrase, and no other content. Please output the lists as a \"code\" block (enclosed by ```...```).\n",
    "```\n",
    "4. Finally, we use part-of-speech tagging (using the `nltk` package) to look for verbs or verb phrases that are in past or future tenses. After the words are tagged with their predicted parts of speech, we use regular expressions (applied to the sequences of tags) to label each verb or verb phrase with a human readable verb form (e.g., \"future perfect continuous passive,\" \"conditional perfect continuous passive,\" and so on).\n",
    "\n",
    "We treat each keyword match (of past or future keywords) as a single \"reference\" (to a past or future event, respectively), and if any past or future verb forms are detected we treat those as (up to) one additional reference.  We then tally up the numbers of past and/or future references across sentences within the document.\n",
    "\n",
    "The `process_folder` function returns two things:\n",
    "  - `df_results` is a DataFrame with one row per document (index), and the following columns:\n",
    "    - `Past`: the number of references to past events identified in the document\n",
    "    - `Future`: the number of references to future events identified in the document\n",
    "  - `sent_results` is a dictionary whose keys are filenames of .txt files in the given folder, and whose values are DataFrames with one row per sentence in the given document.  The per-document DataFrames have the following columns:\n",
    "    - `content`: the text of the given sentence\n",
    "    - `past`: the number of references to past events identified in the given sentence\n",
    "    - `future`: the number of references to future events identified in the given sentence\n",
    "\n",
    "In the metaanalysis reported in our paper, we only use results from the `df_results` DataFrames.  However, the `sent_results` dictionaries are useful for spot-checking how the heuristics are working, and for digging into results for any given document(s).\n",
    "\n",
    "Running the `process_folder` function can take a long time if there are many documents to process.  We save out the results as pickle files after running the function for the first time on a given directory so that the analysis only needs to be run one time per folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keywords(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return set(line.strip() for line in f)\n",
    "\n",
    "\n",
    "def handle_contractions(sentence):\n",
    "    return contractions.fix(sentence)\n",
    "\n",
    "\n",
    "def sentence_tense(x):\n",
    "  # source: https://stackoverflow.com/questions/30016904/determining-tense-of-a-sentence-python\n",
    "  def tense_detect(tagged_sentence):        \n",
    "    verb_tags = ['MD','MDF',\n",
    "                'BE','BEG','BEN','BED','BEDZ','BEZ','BEM','BER',\n",
    "                'DO','DOD','DOZ',\n",
    "                'HV','HVG','HVN','HVD','HVZ',\n",
    "                'VB','VBG','VBN','VBD','VBZ',\n",
    "                'SH',\n",
    "                'TO',                \n",
    "                'JJ']\n",
    "    \n",
    "    verb_phrase = []\n",
    "    for item in tagged_sentence:\n",
    "        if item[1] in verb_tags:\n",
    "            verb_phrase.append(item)\n",
    "\n",
    "    grammar = r'''\n",
    "            future perfect continuous passive:     {<MDF><HV><BEN><BEG><VBN|VBD>+}\n",
    "            conditional perfect continuous passive:{<MD><HV><BEN><BEG><VBN|VBD>+}\n",
    "            future continuous passive:             {<MDF><BE><BEG><VBN|VBD>+}   \n",
    "            conditional continuous passive:        {<MD><BE><BEG><VBN|VBD>+}    \n",
    "            future perfect continuous:             {<MDF><HV><BEN><VBG|HVG|BEG>+}   \n",
    "            conditional perfect continuous:        {<MD><HV><BEN><VBG|HVG|BEG>+}\n",
    "            past perfect continuous passive:       {<HVD><BEN><BEG><VBN|VBD>+}\n",
    "            present perfect continuous passive:    {<HV|HVZ><BEN><BEG><VBN|VBD>+}\n",
    "            future perfect passive:                {<MDF><HV><BEN><VBN|VBD>+}   \n",
    "            conditional perfect passive:           {<MD><HV><BEN><VBN|VBD>+}    \n",
    "            future continuous:                     {<MDF><BE><VBG|HVG|BEG>+ }   \n",
    "            conditional continuous:                {<MD><BE><VBG|HVG|BEG>+  }   \n",
    "            future indefinite passive:             {<MDF><BE><VBN|VBD>+ }\n",
    "            conditional indefinite passive:        {<MD><BE><VBN|VBD>+  }\n",
    "            future perfect:                        {<MDF><HV><HVN|BEN|VBN|VBD>+ }   \n",
    "            conditional perfect:                   {<MD><HV><HVN|BEN|VBN|VBD>+  }   \n",
    "            past continuous passive:               {<BED|BEDZ><BEG><VBN|VBD>+}  \n",
    "            past perfect continuous:               {<HVD><BEN><HVG|BEG|VBG>+}   \n",
    "            past perfect passive:                  {<HVD><BEN><VBN|VBD>+}\n",
    "            present continuous passive:            {<BEM|BER|BEZ><BEG><VBN|VBD>+}   \n",
    "            present perfect continuous:            {<HV|HVZ><BEN><VBG|BEG|HVG>+}    \n",
    "            present perfect passive:               {<HV|HVZ><BEN><VBN|VBD>+}\n",
    "            future indefinite:                     {<MDF><BE|DO|VB|HV>+ }       \n",
    "            conditional indefinite:                {<MD><BE|DO|VB|HV>+  }   \n",
    "            past continuous:                       {<BED|BEDZ><VBG|HVG|BEG>+}           \n",
    "            past perfect:                          {<HVD><BEN|VBN|HVD|HVN>+}\n",
    "            past indefinite passive:               {<BED|BEDZ><VBN|VBD>+}   \n",
    "            present indefinite passive:            {<BEM|BER|BEZ><VBN|VBD>+}            \n",
    "            present continuous:                    {<BEM|BER|BEZ><BEG|VBG|HVG>+}            \n",
    "            present perfect:                       {<HV|HVZ><BEN|HVD|VBN|VBD>+  }       \n",
    "            past indefinite:                       {<DOD><VB|HV|DO>|<BEDZ|BED|HVD|VBN|VBD>+}        \n",
    "            infinitive:                            {<TO><BE|HV|VB>+}\n",
    "            present indefinite:                    {<DO|DOZ><DO|HV|VB>+|<DO|HV|VB|BEZ|DOZ|BER|HVZ|BEM|VBZ>+}    \n",
    "            '''\n",
    "\n",
    "    if len(verb_phrase) > 0:\n",
    "      cp = nltk.RegexpParser(grammar)\n",
    "      result = cp.parse(verb_phrase)\n",
    "    else:\n",
    "      result = []\n",
    "    \n",
    "    tenses_set = set()\n",
    "    for node in result:\n",
    "      if type(node) is nltk.tree.Tree:\n",
    "        tenses_set.add(node.label())\n",
    "    \n",
    "    return tenses_set\n",
    "    \n",
    "  text = word_tokenize(x)\n",
    "  tagged = pos_tag(text)\n",
    "  return tense_detect(tagged)\n",
    "\n",
    "\n",
    "def analyze_sentence(sentence, past_keywords, future_keywords):\n",
    "    past_count = 0\n",
    "    future_count = 0\n",
    "\n",
    "    sentence = handle_contractions(sentence)\n",
    "    \n",
    "    # Check for past and future keywords\n",
    "    past_kw_found = any(keyword in sentence for keyword in past_keywords)\n",
    "    future_kw_found = any(keyword in sentence for keyword in future_keywords)\n",
    "\n",
    "    # Count up to one past and/or future reference based on keywords\n",
    "    past_count += int(past_kw_found)\n",
    "    future_count += int(future_kw_found)\n",
    "    \n",
    "    # Also look at tenses\n",
    "    tenses = sentence_tense(sentence)\n",
    "    if any(['past' in x for x in tenses]):\n",
    "        past_count += 1\n",
    "    if any(['future' in x for x in tenses]) or any(['conditional indefinite' in x for x in tenses]):\n",
    "        future_count += 1\n",
    "\n",
    "    return past_count, future_count\n",
    "\n",
    "\n",
    "def process_folder(folder_path, past_keywords, future_keywords):    \n",
    "    # Dictionary to store results\n",
    "    results_dict = defaultdict(lambda: {\"Past\": 0, \"Future\": 0})\n",
    "    sentence_dfs = {}\n",
    "\n",
    "    for file_name in tqdm(os.listdir(folder_path)):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder_path, file_name), 'r', encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                sentences = nltk.sent_tokenize(content)\n",
    "\n",
    "                # Dataframe to store results for each sentence in the current file\n",
    "                df = pd.DataFrame(columns=[\"content\", \"past\", \"future\"])\n",
    "\n",
    "                for sentence in sentences:\n",
    "                    past_count, future_count = analyze_sentence(sentence, past_keywords, future_keywords)\n",
    "                    results_dict[file_name][\"Past\"] += past_count\n",
    "                    results_dict[file_name][\"Future\"] += future_count\n",
    "                    df = df._append({\"content\": sentence, \"past\": past_count, \"future\": future_count}, ignore_index=True)\n",
    "\n",
    "                sentence_dfs[file_name] = df\n",
    "\n",
    "    df_results = pd.DataFrame(results_dict).T\n",
    "\n",
    "    return df_results, sentence_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords that reflect past events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Past keywords and phrases:**\n",
       "\n",
       "last quarter | so far | olden days | elapsed | once | to date | once upon a time | used to be | wrote | thus far | yesteryear | antiquity | ceased | up to now | recently | previously | already | formerly | since | in those days | last year | said | last semester | expired | had | last night | made | ago | last week | heretofore | before | earlier | last season | last month | final | did | was | in the past | back when | were | long ago | used to | last time | then | concluded | yesterday | bygone | terminated | hitherto | historically"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_keywords = load_keywords(str(Path.cwd().parent.joinpath('data', 'past_keywords.txt')))\n",
    "Markdown('**Past keywords and phrases:**\\n\\n' + ' | '.join(past_keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords that reflect future events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Future keywords and phrases:**\n",
       "\n",
       "could | subsequent | scheduled to | next month | next semester | in the works | soon | intend to | upcoming | shortly | anticipated | next week | next time | eventually | next season | on the horizon | impending | shall | hereafter | in the future | later on | futuristic | looming | forthcoming | next year | plan to | in time | in the cards | to be | prospective | succeeding | some day | can | eventual | predicted | may | tomorrow | later | will | down the line | after | going to | might | next quarter | imminently"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_keywords = load_keywords(str(Path.cwd().parent.joinpath('data', 'future_keywords.txt')))\n",
    "Markdown('**Future keywords and phrases:**\\n\\n' + ' | '.join(future_keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the data\n",
    "\n",
    "We'll examine 12 datasets that span several broad categories of documents: *film* (transcripts of movies or excerpts of transcripts from movies), *television* (transcripts of television shows or excerpts of transcripts from television shows), *speech* (transcripts of spoken communication), and *text* (written works or conversations that took place using text-based media).\n",
    "\n",
    "The datasets are summarized in the DataFrame below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Short name</th>\n",
       "      <th>Data URL</th>\n",
       "      <th>Source URL</th>\n",
       "      <th>Results URL</th>\n",
       "      <th>Description</th>\n",
       "      <th>Category</th>\n",
       "      <th>Number of observations</th>\n",
       "      <th>Observation type</th>\n",
       "      <th>Number of words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Internet Movie Script Database</td>\n",
       "      <td>IMSDb</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/ct39vqqq9sjqyyh...</td>\n",
       "      <td>https://imsdb.com</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/3gq5ieq7l25719i...</td>\n",
       "      <td>A collection of transcripts from roughly 1000 ...</td>\n",
       "      <td>Film</td>\n",
       "      <td>1091</td>\n",
       "      <td>Transcript</td>\n",
       "      <td>26023348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Movie Dialogues Dataset</td>\n",
       "      <td>Movies</td>\n",
       "      <td>https://www.dropbox.com/s/881yuhil48v6q1n/movi...</td>\n",
       "      <td>https://convokit.cornell.edu/documentation/mov...</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/arxkyhub2fi6qh5...</td>\n",
       "      <td>A large collection of fictional conversations ...</td>\n",
       "      <td>Film</td>\n",
       "      <td>304713</td>\n",
       "      <td>Utterance</td>\n",
       "      <td>3209921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Switchboard Dialog Act Corpus</td>\n",
       "      <td>Switchboard</td>\n",
       "      <td>https://www.dropbox.com/s/qvx4211u41l2ex4/swit...</td>\n",
       "      <td>https://convokit.cornell.edu/documentation/swi...</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/1o7wqdlc1oo26y6...</td>\n",
       "      <td>A collection of five-minute telephone conversa...</td>\n",
       "      <td>Speech</td>\n",
       "      <td>122646</td>\n",
       "      <td>Utterance</td>\n",
       "      <td>2052779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Supreme Court Corpus</td>\n",
       "      <td>SCOTUS</td>\n",
       "      <td>https://www.dropbox.com/s/icxk3ubo2u2brzq/supr...</td>\n",
       "      <td>https://convokit.cornell.edu/documentation/sup...</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/zxkvlrg4lfxcv7c...</td>\n",
       "      <td>A collection of cases from the U.S. Supreme Co...</td>\n",
       "      <td>Speech</td>\n",
       "      <td>1700789</td>\n",
       "      <td>Utterance</td>\n",
       "      <td>71889094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tennis Interviews</td>\n",
       "      <td>Tennis</td>\n",
       "      <td>https://www.dropbox.com/s/q7bfirllnu32mao/tenn...</td>\n",
       "      <td>https://convokit.cornell.edu/documentation/ten...</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/d3g83mtz4mqhbpm...</td>\n",
       "      <td>Transcripts for tennis singles post-match pres...</td>\n",
       "      <td>Speech</td>\n",
       "      <td>163948</td>\n",
       "      <td>Utterance</td>\n",
       "      <td>7043118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Persuasion for Good Corpus</td>\n",
       "      <td>PfG</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/ei7uxv9husg9noj...</td>\n",
       "      <td>https://convokit.cornell.edu/documentation/per...</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/zmumd8uno58cqzo...</td>\n",
       "      <td>A collection of online conversations generated...</td>\n",
       "      <td>Speech</td>\n",
       "      <td>20932</td>\n",
       "      <td>Utterance</td>\n",
       "      <td>351759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Intelligence Squared Debates Corpus</td>\n",
       "      <td>IQ2</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/srg1j0m4rhgoqhl...</td>\n",
       "      <td>https://convokit.cornell.edu/documentation/iq2...</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/3d4eha6r6xop7h0...</td>\n",
       "      <td>This dataset contains transcripts of debates h...</td>\n",
       "      <td>Speech</td>\n",
       "      <td>26562</td>\n",
       "      <td>Utterance</td>\n",
       "      <td>1898509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Group Affect and Performance Corpus</td>\n",
       "      <td>GAP</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/j1zh1pey7m8kcyr...</td>\n",
       "      <td>https://convokit.cornell.edu/documentation/gap...</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/prk03sodn4pg895...</td>\n",
       "      <td>Group members completed a Winter Survival Task...</td>\n",
       "      <td>Speech</td>\n",
       "      <td>8009</td>\n",
       "      <td>Utterance</td>\n",
       "      <td>45989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Chair</td>\n",
       "      <td>Chair</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/9cpj3t1n1ktxghu...</td>\n",
       "      <td>https://scrapsfromtheloft.com/?s=THE+CHAIR</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/to0642t939pvrtz...</td>\n",
       "      <td>Scraped transcripts from The Chair, Season 1.</td>\n",
       "      <td>Television</td>\n",
       "      <td>6</td>\n",
       "      <td>Transcript</td>\n",
       "      <td>19197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Friends Corpus</td>\n",
       "      <td>Friends</td>\n",
       "      <td>https://www.dropbox.com/s/nfaa6ap0ws1rqjy/frie...</td>\n",
       "      <td>https://convokit.cornell.edu/documentation/fri...</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/mkxc114g90rifsm...</td>\n",
       "      <td>A collection of all the conversations that occ...</td>\n",
       "      <td>Television</td>\n",
       "      <td>67373</td>\n",
       "      <td>Utterance</td>\n",
       "      <td>622894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gutenberg Dialogue Dataset</td>\n",
       "      <td>Gutenberg</td>\n",
       "      <td>https://www.dropbox.com/s/84rid3cboynutmr/gute...</td>\n",
       "      <td>https://github.com/ricsinaruto/gutenberg-dialog</td>\n",
       "      <td>https://www.dropbox.com/s/jz15wcsceacaqva/gute...</td>\n",
       "      <td>Dialogues extracted from the Project Gutenberg...</td>\n",
       "      <td>Text</td>\n",
       "      <td>14773741</td>\n",
       "      <td>Utterance</td>\n",
       "      <td>327519461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Reddit Corpus</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>https://www.dropbox.com/s/k7cun7f6x2guwva/redd...</td>\n",
       "      <td>https://convokit.cornell.edu/documentation/sub...</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/p999uknmzhx6f49...</td>\n",
       "      <td>A collection of Corpuses of Reddit data built ...</td>\n",
       "      <td>Text</td>\n",
       "      <td>74468</td>\n",
       "      <td>Utterance</td>\n",
       "      <td>3080662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Dataset   Short name   \n",
       "0        Internet Movie Script Database        IMSDb  \\\n",
       "1               Movie Dialogues Dataset       Movies   \n",
       "2         Switchboard Dialog Act Corpus  Switchboard   \n",
       "3                  Supreme Court Corpus       SCOTUS   \n",
       "4                     Tennis Interviews       Tennis   \n",
       "5            Persuasion for Good Corpus          PfG   \n",
       "6   Intelligence Squared Debates Corpus          IQ2   \n",
       "7   Group Affect and Performance Corpus          GAP   \n",
       "8                             The Chair        Chair   \n",
       "9                        Friends Corpus      Friends   \n",
       "10           Gutenberg Dialogue Dataset    Gutenberg   \n",
       "11                        Reddit Corpus       Reddit   \n",
       "\n",
       "                                             Data URL   \n",
       "0   https://www.dropbox.com/scl/fi/ct39vqqq9sjqyyh...  \\\n",
       "1   https://www.dropbox.com/s/881yuhil48v6q1n/movi...   \n",
       "2   https://www.dropbox.com/s/qvx4211u41l2ex4/swit...   \n",
       "3   https://www.dropbox.com/s/icxk3ubo2u2brzq/supr...   \n",
       "4   https://www.dropbox.com/s/q7bfirllnu32mao/tenn...   \n",
       "5   https://www.dropbox.com/scl/fi/ei7uxv9husg9noj...   \n",
       "6   https://www.dropbox.com/scl/fi/srg1j0m4rhgoqhl...   \n",
       "7   https://www.dropbox.com/scl/fi/j1zh1pey7m8kcyr...   \n",
       "8   https://www.dropbox.com/scl/fi/9cpj3t1n1ktxghu...   \n",
       "9   https://www.dropbox.com/s/nfaa6ap0ws1rqjy/frie...   \n",
       "10  https://www.dropbox.com/s/84rid3cboynutmr/gute...   \n",
       "11  https://www.dropbox.com/s/k7cun7f6x2guwva/redd...   \n",
       "\n",
       "                                           Source URL   \n",
       "0                                   https://imsdb.com  \\\n",
       "1   https://convokit.cornell.edu/documentation/mov...   \n",
       "2   https://convokit.cornell.edu/documentation/swi...   \n",
       "3   https://convokit.cornell.edu/documentation/sup...   \n",
       "4   https://convokit.cornell.edu/documentation/ten...   \n",
       "5   https://convokit.cornell.edu/documentation/per...   \n",
       "6   https://convokit.cornell.edu/documentation/iq2...   \n",
       "7   https://convokit.cornell.edu/documentation/gap...   \n",
       "8          https://scrapsfromtheloft.com/?s=THE+CHAIR   \n",
       "9   https://convokit.cornell.edu/documentation/fri...   \n",
       "10    https://github.com/ricsinaruto/gutenberg-dialog   \n",
       "11  https://convokit.cornell.edu/documentation/sub...   \n",
       "\n",
       "                                          Results URL   \n",
       "0   https://www.dropbox.com/scl/fi/3gq5ieq7l25719i...  \\\n",
       "1   https://www.dropbox.com/scl/fi/arxkyhub2fi6qh5...   \n",
       "2   https://www.dropbox.com/scl/fi/1o7wqdlc1oo26y6...   \n",
       "3   https://www.dropbox.com/scl/fi/zxkvlrg4lfxcv7c...   \n",
       "4   https://www.dropbox.com/scl/fi/d3g83mtz4mqhbpm...   \n",
       "5   https://www.dropbox.com/scl/fi/zmumd8uno58cqzo...   \n",
       "6   https://www.dropbox.com/scl/fi/3d4eha6r6xop7h0...   \n",
       "7   https://www.dropbox.com/scl/fi/prk03sodn4pg895...   \n",
       "8   https://www.dropbox.com/scl/fi/to0642t939pvrtz...   \n",
       "9   https://www.dropbox.com/scl/fi/mkxc114g90rifsm...   \n",
       "10  https://www.dropbox.com/s/jz15wcsceacaqva/gute...   \n",
       "11  https://www.dropbox.com/scl/fi/p999uknmzhx6f49...   \n",
       "\n",
       "                                          Description    Category   \n",
       "0   A collection of transcripts from roughly 1000 ...        Film  \\\n",
       "1   A large collection of fictional conversations ...        Film   \n",
       "2   A collection of five-minute telephone conversa...      Speech   \n",
       "3   A collection of cases from the U.S. Supreme Co...      Speech   \n",
       "4   Transcripts for tennis singles post-match pres...      Speech   \n",
       "5   A collection of online conversations generated...      Speech   \n",
       "6   This dataset contains transcripts of debates h...      Speech   \n",
       "7   Group members completed a Winter Survival Task...      Speech   \n",
       "8       Scraped transcripts from The Chair, Season 1.  Television   \n",
       "9   A collection of all the conversations that occ...  Television   \n",
       "10  Dialogues extracted from the Project Gutenberg...        Text   \n",
       "11  A collection of Corpuses of Reddit data built ...        Text   \n",
       "\n",
       "    Number of observations Observation type  Number of words  \n",
       "0                     1091       Transcript         26023348  \n",
       "1                   304713        Utterance          3209921  \n",
       "2                   122646        Utterance          2052779  \n",
       "3                  1700789        Utterance         71889094  \n",
       "4                   163948        Utterance          7043118  \n",
       "5                    20932        Utterance           351759  \n",
       "6                    26562        Utterance          1898509  \n",
       "7                     8009        Utterance            45989  \n",
       "8                        6       Transcript            19197  \n",
       "9                    67373        Utterance           622894  \n",
       "10                14773741        Utterance        327519461  \n",
       "11                   74468        Utterance          3080662  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = Path.cwd().parent.joinpath('data', 'metaanalysis-datasets.xlsx')\n",
    "data = pd.read_excel(data_list)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect past and future events\n",
    "\n",
    "For each dataset in the `data` DataFrame, we'll:\n",
    "  - Download and extract the dataset if it doesn't already exist locally\n",
    "  - Check to see whether the metaanalysis has already been run on that folder.  If not, we'll run the `process_folder` function on the dataset's directory and save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_name(url):\n",
    "    return [s for s in url.split('/') if '.zip' in s][0].split('?')[0][:-4]\n",
    "\n",
    "def download_dataset(url, outdir):\n",
    "    # Download dataset\n",
    "    filename = get_folder_name(url) + '.zip'\n",
    "    x = requests.get(url)\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(x.content)\n",
    "    \n",
    "    # Unzip dataset\n",
    "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(outdir)\n",
    "\n",
    "    # Delete zip file\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: Internet Movie Script Database\n",
      "Processing dataset: Movie Dialogues Dataset\n",
      "Processing dataset: Switchboard Dialog Act Corpus\n",
      "Processing dataset: Supreme Court Corpus\n",
      "Processing dataset: Tennis Interviews\n",
      "Processing dataset: Persuasion for Good Corpus\n",
      "Processing dataset: Intelligence Squared Debates Corpus\n",
      "Processing dataset: Group Affect and Performance Corpus\n",
      "Processing dataset: The Chair\n",
      "Processing dataset: Friends Corpus\n",
      "Processing dataset: Gutenberg Dialogue Dataset\n",
      "Processing dataset: Reddit Corpus\n"
     ]
    }
   ],
   "source": [
    "datadir = Path.cwd().parent.joinpath('data')\n",
    "results = []\n",
    "\n",
    "# should we just download the already-completed results or compute them from scratch?\n",
    "force_rerun = False\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    print('Processing dataset: ' + row['Dataset'])\n",
    "    results_fname = datadir.joinpath(row['Short name'].lower() + '_results.pkl')\n",
    "    if not results_fname.exists():\n",
    "        if force_rerun:\n",
    "            # check whether the dataset exists locally and has at least 5 .txt files\n",
    "            next_datadir = datadir.joinpath(get_folder_name(row['Data URL']))\n",
    "            if not (next_datadir.exists() and len(lsdir(str(next_datadir.joinpath('*.txt')))) >= 5):\n",
    "                # download the dataset\n",
    "                download_dataset(row['Data URL'], datadir)\n",
    "            \n",
    "            # process the dataset\n",
    "            df_results, sentence_dfs = process_folder(next_datadir, past_keywords, future_keywords)\n",
    "            with open(results_fname, 'wb') as f:\n",
    "                pickle.dump([df_results, sentence_dfs], f)\n",
    "        else:\n",
    "            x = requests.get(row['Results URL'])\n",
    "            with open(results_fname, 'wb') as f:\n",
    "                f.write(x.content)\n",
    "    \n",
    "    with open(results_fname, 'rb') as f:\n",
    "        next_results, _ = pickle.load(f)\n",
    "    \n",
    "    next_results = next_results.reset_index().rename(columns={\"index\": \"filename\"}).melt(id_vars=[\"filename\"], var_name=\"tense\", value_name=\"count\")\n",
    "    next_results['proportion'] = next_results['count'] / next_results.groupby('filename')['count'].transform('sum')\n",
    "    next_results['Dataset'] = row['Short name']\n",
    "\n",
    "    results.append(next_results)\n",
    "\n",
    "results = pd.concat(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in manual counts for *The Chair*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Episode</th>\n",
       "      <th>tense</th>\n",
       "      <th>count</th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Past</td>\n",
       "      <td>60</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Future</td>\n",
       "      <td>18</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Past</td>\n",
       "      <td>30</td>\n",
       "      <td>0.681818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Future</td>\n",
       "      <td>14</td>\n",
       "      <td>0.318182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Past</td>\n",
       "      <td>43</td>\n",
       "      <td>0.565789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>Future</td>\n",
       "      <td>33</td>\n",
       "      <td>0.434211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>Past</td>\n",
       "      <td>31</td>\n",
       "      <td>0.596154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>Future</td>\n",
       "      <td>21</td>\n",
       "      <td>0.403846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>Past</td>\n",
       "      <td>36</td>\n",
       "      <td>0.765957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>Future</td>\n",
       "      <td>11</td>\n",
       "      <td>0.234043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>Past</td>\n",
       "      <td>27</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>Future</td>\n",
       "      <td>12</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Episode   tense  count  proportion\n",
       "0         1    Past     60    0.769231\n",
       "1         1  Future     18    0.230769\n",
       "2         2    Past     30    0.681818\n",
       "3         2  Future     14    0.318182\n",
       "4         3    Past     43    0.565789\n",
       "5         3  Future     33    0.434211\n",
       "6         4    Past     31    0.596154\n",
       "7         4  Future     21    0.403846\n",
       "8         5    Past     36    0.765957\n",
       "9         5  Future     11    0.234043\n",
       "10        6    Past     27    0.692308\n",
       "11        6  Future     12    0.307692"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill in proportions for manual reference counts\n",
    "ref_fname = str(Path.cwd().parent.joinpath('data', 'the_chair', 'the_chair_manual_reference_counts.csv'))\n",
    "manual = pd.read_csv(ref_fname)\n",
    "manual['Total'] = manual['Past'] + manual['Future']\n",
    "\n",
    "# # compute proportions\n",
    "# manual['p(Past)'] = manual['Past'] / manual['Total']\n",
    "# manual['p(Future)'] = manual['Future'] / manual['Total']\n",
    "\n",
    "manual.reset_index(inplace=True)\n",
    "manual['Episode'] = manual['index'] + 1\n",
    "manual.drop(['index', 'Total'], axis=1, inplace=True)\n",
    "\n",
    "manual = manual.melt(var_name='tense', value_name='count', id_vars=['Episode'])\n",
    "manual.sort_values(['Episode'], inplace=True)\n",
    "manual.reset_index(inplace=True, drop=True)\n",
    "manual['proportion'] = manual['count'] / manual.groupby('Episode')['count'].transform('sum')\n",
    "manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load automatically identified counts from *The Chair*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tense</th>\n",
       "      <th>count</th>\n",
       "      <th>Episode</th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Past</td>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "      <td>0.575758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Future</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>0.424242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Past</td>\n",
       "      <td>108</td>\n",
       "      <td>2</td>\n",
       "      <td>0.529412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Future</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Past</td>\n",
       "      <td>177</td>\n",
       "      <td>3</td>\n",
       "      <td>0.608247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Future</td>\n",
       "      <td>114</td>\n",
       "      <td>3</td>\n",
       "      <td>0.391753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Past</td>\n",
       "      <td>148</td>\n",
       "      <td>4</td>\n",
       "      <td>0.594378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Future</td>\n",
       "      <td>101</td>\n",
       "      <td>4</td>\n",
       "      <td>0.405622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Past</td>\n",
       "      <td>164</td>\n",
       "      <td>5</td>\n",
       "      <td>0.616541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Future</td>\n",
       "      <td>102</td>\n",
       "      <td>5</td>\n",
       "      <td>0.383459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Past</td>\n",
       "      <td>160</td>\n",
       "      <td>6</td>\n",
       "      <td>0.536913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Future</td>\n",
       "      <td>138</td>\n",
       "      <td>6</td>\n",
       "      <td>0.463087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tense  count  Episode  proportion\n",
       "0     Past    152        1    0.575758\n",
       "1   Future    112        1    0.424242\n",
       "2     Past    108        2    0.529412\n",
       "3   Future     96        2    0.470588\n",
       "4     Past    177        3    0.608247\n",
       "5   Future    114        3    0.391753\n",
       "6     Past    148        4    0.594378\n",
       "7   Future    101        4    0.405622\n",
       "8     Past    164        5    0.616541\n",
       "9   Future    102        5    0.383459\n",
       "10    Past    160        6    0.536913\n",
       "11  Future    138        6    0.463087"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chair_fname = str(Path.cwd().parent.joinpath('data', 'chair_results.pkl'))\n",
    "with open(chair_fname, 'rb') as f:\n",
    "    chair_results, _ = pickle.load(f)\n",
    "\n",
    "auto = chair_results.reset_index().rename(columns={\"index\": \"filename\"}).melt(id_vars=[\"filename\"], var_name=\"tense\", value_name=\"count\")\n",
    "auto['Episode'] = auto['filename'].apply(lambda filename: int(filename.split('_')[2][3]))\n",
    "auto['proportion'] = auto['count'] / auto.groupby('Episode')['count'].transform('sum')\n",
    "auto.sort_values(by=['Episode'], inplace=True)\n",
    "auto.drop(columns=['filename'], inplace=True)\n",
    "auto.reset_index(drop=True, inplace=True)\n",
    "auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create meta-analysis figure\n",
    "\n",
    "- Panel A: Numbers of (manually and automatically detected) past and future events from each episode of *The Chair*, season 1.\n",
    "- Panel B: Proportions of (manually and automatically detected) past and future events from each episode of *The Chair*, season 1.\n",
    "- Panel C: Proportions of automatically detected past and future events from each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 4), ncols=3)\n",
    "\n",
    "# panel A\n",
    "sns.barplot(data=manual, x='tense', y='count', hue='Episode', palette='viridis', ax=axes[0])\n",
    "sns.barplot(data=auto, x='tense', y='count', hue='Episode', palette='viridis', alpha=0.5, ax=axes[0])\n",
    "sns.barplot(data=auto, x='tense', y='count', hue='Episode', fill=False, edgecolor='k', linewidth=1.5, ax=axes[0])\n",
    "axes[0].get_legend().remove()\n",
    "\n",
    "axes[0].set_xlabel('Tense', fontsize=14)\n",
    "axes[0].set_ylabel('Number of events', fontsize=14)\n",
    "sns.despine(top=True, right=True)\n",
    "\n",
    "\n",
    "# panel B\n",
    "sns.barplot(data=manual, x='tense', y='proportion', hue='Episode', palette='viridis', ax=axes[1])\n",
    "sns.barplot(data=auto, x='tense', y='proportion', hue='Episode', palette='viridis', alpha=0.5, ax=axes[1])\n",
    "sns.barplot(data=auto, x='tense', y='proportion', hue='Episode', fill=False, edgecolor='k', linewidth=1.5, ax=axes[1])\n",
    "handles, labels = axes[1].get_legend_handles_labels()\n",
    "axes[1].legend(loc='upper right', title='Episode', handles=handles[:6], labels=labels[:6], frameon=True, framealpha=0.75, ncol=2, fontsize=8)\n",
    "\n",
    "axes[1].set_xlabel('Tense', fontsize=14)\n",
    "axes[1].set_ylabel('Proportion of events', fontsize=14)\n",
    "sns.despine(top=True, right=True)\n",
    "\n",
    "\n",
    "# panel C\n",
    "sns.barplot(results, x='tense', y='proportion', hue='Dataset', palette='Spectral', ax=axes[2])\n",
    "axes[2].set_xlabel('Tense', fontsize=14)\n",
    "axes[2].set_ylabel('Proportion of events', fontsize=14)\n",
    "axes[2].legend(loc='lower left', title='Dataset', frameon=True, ncol=3, fontsize=8, facecolor='white', framealpha=0.75)\n",
    "axis[2].set_ylim(axes[1].get_ylim())\n",
    "sns.despine(top=True, right=True)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('meta-analysis.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prediction-retrodiction",
   "language": "python",
   "name": "prediction-retrodiction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
