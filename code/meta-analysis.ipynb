{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import davos\n",
    "except:\n",
    "    %pip install davos\n",
    "    import davos\n",
    "davos.config.suppress_stdout = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "smuggle requests                                            # pip: requests==2.28.2\n",
    "from tqdm smuggle tqdm                                      # pip: tqdm==4.65.0\n",
    "smuggle pandas as pd                                        # pip: pandas==2.0.1\n",
    "smuggle numpy as np                                         # pip: numpy==1.25.2\n",
    "smuggle seaborn as sns                                      # pip: seaborn==0.12.2\n",
    "from matplotlib smuggle pyplot as plt                       # pip: matplotlib==3.7.1\n",
    "from IPython.display smuggle Markdown\n",
    "smuggle openpyxl                                            # pip: openpyxl==3.1.2\n",
    "smuggle contractions                                        # pip: contractions==0.1.73\n",
    "from nltk.tokenize smuggle word_tokenize, sent_tokenize     # pip: nltk==3.8.1\n",
    "from nltk smuggle pos_tag\n",
    "from scipy.stats import ttest_1samp                         # pip: scipy==1.11.2\n",
    "\n",
    "smuggle nltk\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "smuggle re\n",
    "smuggle os\n",
    "smuggle urllib\n",
    "smuggle json\n",
    "smuggle string\n",
    "smuggle warnings\n",
    "smuggle pickle\n",
    "smuggle zipfile\n",
    "from glob smuggle glob as lsdir\n",
    "from collections import defaultdict\n",
    "from pathlib smuggle Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting references to past and future events\n",
    "\n",
    "At a high level, the goal of this meta analysis is to predict in-text references to past and future events.  Manually identifying these references is labor and time intensive, so it is impractical to scale up manual tagging to millions of documents.  Instead, we've defined some heuristics for *predicting* when text is referring to real or hypothetical past or future events.  Our approach comprises four main steps:\n",
    "\n",
    "1. First we use the `nltk` package to segment each document into individual sentences. Each sentence is processed independently of the others.\n",
    "2. Next, we handle contractions using the `contractions` package (e.g., \"we'll\" gets split into \"we will,\" and so on).\n",
    "3. Third, we define two sets of \"keywords\" (words and phrases) that tend to be indicative of referring to the past (`past_keywords.txt`) or future (`future_keywords.txt`).  We used ChatGPT (`gpt-4`) to generate each list, with exactly 50 templates per list, using the following prompt:\n",
    "```\n",
    "I'm designing a heuristic algorithm for identifying references (in text) to past and future events. Part of the algorithm will involve looking for specific keywords or phrases that suggest that the text is referring to something that happened (or will happen) in the past and/or future. Could you help me generate a list of 50 keywords or phrases to include in each list (one list for identifying references to the past and a second list for identifying references to the future)? I'd like to be able to paste the lists you generate into two plain text documents with one row per keyword or phrase, and no other content. Please output the lists as a \"code\" block (enclosed by ```...```).\n",
    "```\n",
    "4. Finally, we use part-of-speech tagging (using the `nltk` package) to look for verbs or verb phrases that are in past or future tenses. After the words are tagged with their predicted parts of speech, we use regular expressions (applied to the sequences of tags) to label each verb or verb phrase with a human readable verb form (e.g., \"future perfect continuous passive,\" \"conditional perfect continuous passive,\" and so on).\n",
    "\n",
    "We treat each keyword match (of past or future keywords) as a single \"reference\" (to a past or future event, respectively), and if any past or future verb forms are detected we treat those as (up to) one additional reference.  We then tally up the numbers of past and/or future references across sentences within the document.\n",
    "\n",
    "The `process_folder` function returns two things:\n",
    "  - `df_results` is a DataFrame with one row per document (index), and the following columns:\n",
    "    - `Past`: the number of references to past events identified in the document\n",
    "    - `Future`: the number of references to future events identified in the document\n",
    "  - `sent_results` is a dictionary whose keys are filenames of .txt files in the given folder, and whose values are DataFrames with one row per sentence in the given document.  The per-document DataFrames have the following columns:\n",
    "    - `content`: the text of the given sentence\n",
    "    - `past`: the number of references to past events identified in the given sentence\n",
    "    - `future`: the number of references to future events identified in the given sentence\n",
    "\n",
    "In the metaanalysis reported in our paper, we only use results from the `df_results` DataFrames.  However, the `sent_results` dictionaries are useful for spot-checking how the heuristics are working, and for digging into results for any given document(s).\n",
    "\n",
    "Running the `process_folder` function can take a long time if there are many documents to process.  We save out the results as pickle files after running the function for the first time on a given directory so that the analysis only needs to be run one time per folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keywords(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return set(line.strip() for line in f)\n",
    "\n",
    "\n",
    "def handle_contractions(sentence):\n",
    "    return contractions.fix(sentence)\n",
    "\n",
    "\n",
    "def sentence_tense(x):\n",
    "  # source: https://stackoverflow.com/questions/30016904/determining-tense-of-a-sentence-python\n",
    "  def tense_detect(tagged_sentence):        \n",
    "    verb_tags = ['MD','MDF',\n",
    "                'BE','BEG','BEN','BED','BEDZ','BEZ','BEM','BER',\n",
    "                'DO','DOD','DOZ',\n",
    "                'HV','HVG','HVN','HVD','HVZ',\n",
    "                'VB','VBG','VBN','VBD','VBZ',\n",
    "                'SH',\n",
    "                'TO',                \n",
    "                'JJ']\n",
    "    \n",
    "    verb_phrase = []\n",
    "    for item in tagged_sentence:\n",
    "        if item[1] in verb_tags:\n",
    "            verb_phrase.append(item)\n",
    "\n",
    "    grammar = r'''\n",
    "            future perfect continuous passive:     {<MDF><HV><BEN><BEG><VBN|VBD>+}\n",
    "            conditional perfect continuous passive:{<MD><HV><BEN><BEG><VBN|VBD>+}\n",
    "            future continuous passive:             {<MDF><BE><BEG><VBN|VBD>+}   \n",
    "            conditional continuous passive:        {<MD><BE><BEG><VBN|VBD>+}    \n",
    "            future perfect continuous:             {<MDF><HV><BEN><VBG|HVG|BEG>+}   \n",
    "            conditional perfect continuous:        {<MD><HV><BEN><VBG|HVG|BEG>+}\n",
    "            past perfect continuous passive:       {<HVD><BEN><BEG><VBN|VBD>+}\n",
    "            present perfect continuous passive:    {<HV|HVZ><BEN><BEG><VBN|VBD>+}\n",
    "            future perfect passive:                {<MDF><HV><BEN><VBN|VBD>+}   \n",
    "            conditional perfect passive:           {<MD><HV><BEN><VBN|VBD>+}    \n",
    "            future continuous:                     {<MDF><BE><VBG|HVG|BEG>+ }   \n",
    "            conditional continuous:                {<MD><BE><VBG|HVG|BEG>+  }   \n",
    "            future indefinite passive:             {<MDF><BE><VBN|VBD>+ }\n",
    "            conditional indefinite passive:        {<MD><BE><VBN|VBD>+  }\n",
    "            future perfect:                        {<MDF><HV><HVN|BEN|VBN|VBD>+ }   \n",
    "            conditional perfect:                   {<MD><HV><HVN|BEN|VBN|VBD>+  }   \n",
    "            past continuous passive:               {<BED|BEDZ><BEG><VBN|VBD>+}  \n",
    "            past perfect continuous:               {<HVD><BEN><HVG|BEG|VBG>+}   \n",
    "            past perfect passive:                  {<HVD><BEN><VBN|VBD>+}\n",
    "            present continuous passive:            {<BEM|BER|BEZ><BEG><VBN|VBD>+}   \n",
    "            present perfect continuous:            {<HV|HVZ><BEN><VBG|BEG|HVG>+}    \n",
    "            present perfect passive:               {<HV|HVZ><BEN><VBN|VBD>+}\n",
    "            future indefinite:                     {<MDF><BE|DO|VB|HV>+ }       \n",
    "            conditional indefinite:                {<MD><BE|DO|VB|HV>+  }   \n",
    "            past continuous:                       {<BED|BEDZ><VBG|HVG|BEG>+}           \n",
    "            past perfect:                          {<HVD><BEN|VBN|HVD|HVN>+}\n",
    "            past indefinite passive:               {<BED|BEDZ><VBN|VBD>+}   \n",
    "            present indefinite passive:            {<BEM|BER|BEZ><VBN|VBD>+}            \n",
    "            present continuous:                    {<BEM|BER|BEZ><BEG|VBG|HVG>+}            \n",
    "            present perfect:                       {<HV|HVZ><BEN|HVD|VBN|VBD>+  }       \n",
    "            past indefinite:                       {<DOD><VB|HV|DO>|<BEDZ|BED|HVD|VBN|VBD>+}        \n",
    "            infinitive:                            {<TO><BE|HV|VB>+}\n",
    "            present indefinite:                    {<DO|DOZ><DO|HV|VB>+|<DO|HV|VB|BEZ|DOZ|BER|HVZ|BEM|VBZ>+}    \n",
    "            '''\n",
    "\n",
    "    if len(verb_phrase) > 0:\n",
    "      cp = nltk.RegexpParser(grammar)\n",
    "      result = cp.parse(verb_phrase)\n",
    "    else:\n",
    "      result = []\n",
    "    \n",
    "    tenses_set = set()\n",
    "    for node in result:\n",
    "      if type(node) is nltk.tree.Tree:\n",
    "        tenses_set.add(node.label())\n",
    "    \n",
    "    return tenses_set\n",
    "    \n",
    "  text = word_tokenize(x)\n",
    "  tagged = pos_tag(text)\n",
    "  return tense_detect(tagged)\n",
    "\n",
    "\n",
    "def analyze_sentence(sentence, past_keywords, future_keywords):\n",
    "    past_count = 0\n",
    "    future_count = 0\n",
    "\n",
    "    sentence = handle_contractions(sentence)\n",
    "    \n",
    "    # Check for past and future keywords\n",
    "    past_kw_found = any(keyword in sentence for keyword in past_keywords)\n",
    "    future_kw_found = any(keyword in sentence for keyword in future_keywords)\n",
    "\n",
    "    # Count up to one past and/or future reference based on keywords\n",
    "    past_count += int(past_kw_found)\n",
    "    future_count += int(future_kw_found)\n",
    "    \n",
    "    # Also look at tenses\n",
    "    tenses = sentence_tense(sentence)\n",
    "    if any(['past' in x for x in tenses]):\n",
    "        past_count += 1\n",
    "    if any(['future' in x for x in tenses]) or any(['conditional indefinite' in x for x in tenses]):\n",
    "        future_count += 1\n",
    "\n",
    "    return past_count, future_count\n",
    "\n",
    "\n",
    "def process_folder(folder_path, past_keywords, future_keywords):    \n",
    "    # Dictionary to store results\n",
    "    results_dict = defaultdict(lambda: {\"Past\": 0, \"Future\": 0})\n",
    "    sentence_dfs = {}\n",
    "\n",
    "    for file_name in tqdm(os.listdir(folder_path)):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder_path, file_name), 'r', encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "                sentences = nltk.sent_tokenize(content)\n",
    "\n",
    "                # Dataframe to store results for each sentence in the current file\n",
    "                df = pd.DataFrame(columns=[\"content\", \"past\", \"future\"])\n",
    "\n",
    "                for sentence in sentences:\n",
    "                    past_count, future_count = analyze_sentence(sentence, past_keywords, future_keywords)\n",
    "                    results_dict[file_name][\"Past\"] += past_count\n",
    "                    results_dict[file_name][\"Future\"] += future_count\n",
    "                    df = df._append({\"content\": sentence, \"past\": past_count, \"future\": future_count}, ignore_index=True)\n",
    "\n",
    "                sentence_dfs[file_name] = df\n",
    "\n",
    "    df_results = pd.DataFrame(results_dict).T\n",
    "    \n",
    "    return df_results, sentence_dfs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords that reflect past events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Past keywords and phrases:**\n",
       "\n",
       "final | so far | used to be | to date | had | made | last night | long ago | already | last season | concluded | were | once | previously | last month | ceased | earlier | in the past | before | said | up to now | heretofore | last year | wrote | terminated | last semester | yesteryear | was | antiquity | last time | since | in those days | did | thus far | back when | last quarter | ago | formerly | elapsed | olden days | yesterday | recently | once upon a time | then | expired | hitherto | used to | historically | last week | bygone"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_keywords = load_keywords(str(Path.cwd().parent.joinpath('data', 'past_keywords.txt')))\n",
    "Markdown('**Past keywords and phrases:**\\n\\n' + ' | '.join(past_keywords))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords that reflect future events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Future keywords and phrases:**\n",
       "\n",
       "to be | prospective | futuristic | next time | tomorrow | on the horizon | imminently | next quarter | forthcoming | soon | next year | next season | could | subsequent | impending | can | down the line | in time | eventual | later on | going to | predicted | may | in the future | some day | might | succeeding | anticipated | shall | next week | looming | scheduled to | later | in the cards | intend to | eventually | hereafter | upcoming | after | will | next semester | shortly | in the works | next month | plan to"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_keywords = load_keywords(str(Path.cwd().parent.joinpath('data', 'future_keywords.txt')))\n",
    "Markdown('**Future keywords and phrases:**\\n\\n' + ' | '.join(future_keywords))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the data\n",
    "\n",
    "We'll examine 12 datasets that span several broad categories of documents: *film* (transcripts of movies or excerpts of transcripts from movies), *television* (transcripts of television shows or excerpts of transcripts from television shows), *speech* (transcripts of spoken communication), and *text* (written works or conversations that took place using text-based media).\n",
    "\n",
    "The datasets are summarized in the DataFrame below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Short name</th>\n",
       "      <th>Data URL</th>\n",
       "      <th>Source URL</th>\n",
       "      <th>Results URL</th>\n",
       "      <th>Description</th>\n",
       "      <th>Category</th>\n",
       "      <th>Number of observations</th>\n",
       "      <th>Observation type</th>\n",
       "      <th>Number of words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Internet Movie Script Database</td>\n",
       "      <td>IMSDb</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/ct39vqqq9sjqyyh...</td>\n",
       "      <td>https://imsdb.com</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/3gq5ieq7l25719i...</td>\n",
       "      <td>A collection of transcripts from roughly 1000 ...</td>\n",
       "      <td>Film</td>\n",
       "      <td>1091</td>\n",
       "      <td>Transcript</td>\n",
       "      <td>26023348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Movie Dialogues Dataset</td>\n",
       "      <td>Movies</td>\n",
       "      <td>https://www.dropbox.com/s/881yuhil48v6q1n/movi...</td>\n",
       "      <td>https://convokit.cornell.edu/documentation/mov...</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/arxkyhub2fi6qh5...</td>\n",
       "      <td>A large collection of fictional conversations ...</td>\n",
       "      <td>Film</td>\n",
       "      <td>304713</td>\n",
       "      <td>Utterance</td>\n",
       "      <td>3209921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Switchboard Dialog Act Corpus</td>\n",
       "      <td>Switchboard</td>\n",
       "      <td>https://www.dropbox.com/s/qvx4211u41l2ex4/swit...</td>\n",
       "      <td>https://convokit.cornell.edu/documentation/swi...</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/1o7wqdlc1oo26y6...</td>\n",
       "      <td>A collection of five-minute telephone conversa...</td>\n",
       "      <td>Speech</td>\n",
       "      <td>122646</td>\n",
       "      <td>Utterance</td>\n",
       "      <td>2052779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Supreme Court Corpus</td>\n",
       "      <td>SCOTUS</td>\n",
       "      <td>https://www.dropbox.com/s/icxk3ubo2u2brzq/supr...</td>\n",
       "      <td>https://convokit.cornell.edu/documentation/sup...</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/zxkvlrg4lfxcv7c...</td>\n",
       "      <td>A collection of cases from the U.S. Supreme Co...</td>\n",
       "      <td>Speech</td>\n",
       "      <td>1700789</td>\n",
       "      <td>Utterance</td>\n",
       "      <td>71889094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tennis Interviews</td>\n",
       "      <td>Tennis</td>\n",
       "      <td>https://www.dropbox.com/s/q7bfirllnu32mao/tenn...</td>\n",
       "      <td>https://convokit.cornell.edu/documentation/ten...</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/d3g83mtz4mqhbpm...</td>\n",
       "      <td>Transcripts for tennis singles post-match pres...</td>\n",
       "      <td>Speech</td>\n",
       "      <td>163948</td>\n",
       "      <td>Utterance</td>\n",
       "      <td>7043118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Persuasion for Good Corpus</td>\n",
       "      <td>PfG</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/ei7uxv9husg9noj...</td>\n",
       "      <td>https://convokit.cornell.edu/documentation/per...</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/zmumd8uno58cqzo...</td>\n",
       "      <td>A collection of online conversations generated...</td>\n",
       "      <td>Speech</td>\n",
       "      <td>20932</td>\n",
       "      <td>Utterance</td>\n",
       "      <td>351759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Intelligence Squared Debates Corpus</td>\n",
       "      <td>IQ2</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/srg1j0m4rhgoqhl...</td>\n",
       "      <td>https://convokit.cornell.edu/documentation/iq2...</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/3d4eha6r6xop7h0...</td>\n",
       "      <td>This dataset contains transcripts of debates h...</td>\n",
       "      <td>Speech</td>\n",
       "      <td>26562</td>\n",
       "      <td>Utterance</td>\n",
       "      <td>1898509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Group Affect and Performance Corpus</td>\n",
       "      <td>GAP</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/j1zh1pey7m8kcyr...</td>\n",
       "      <td>https://convokit.cornell.edu/documentation/gap...</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/prk03sodn4pg895...</td>\n",
       "      <td>Group members completed a Winter Survival Task...</td>\n",
       "      <td>Speech</td>\n",
       "      <td>8009</td>\n",
       "      <td>Utterance</td>\n",
       "      <td>45989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Chair</td>\n",
       "      <td>Chair</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/9cpj3t1n1ktxghu...</td>\n",
       "      <td>https://scrapsfromtheloft.com/?s=THE+CHAIR</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/to0642t939pvrtz...</td>\n",
       "      <td>Scraped transcripts from The Chair, Season 1.</td>\n",
       "      <td>Television</td>\n",
       "      <td>6</td>\n",
       "      <td>Transcript</td>\n",
       "      <td>19197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Friends Corpus</td>\n",
       "      <td>Friends</td>\n",
       "      <td>https://www.dropbox.com/s/nfaa6ap0ws1rqjy/frie...</td>\n",
       "      <td>https://convokit.cornell.edu/documentation/fri...</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/mkxc114g90rifsm...</td>\n",
       "      <td>A collection of all the conversations that occ...</td>\n",
       "      <td>Television</td>\n",
       "      <td>67373</td>\n",
       "      <td>Utterance</td>\n",
       "      <td>622894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gutenberg Dialogue Dataset</td>\n",
       "      <td>Gutenberg</td>\n",
       "      <td>https://www.dropbox.com/s/84rid3cboynutmr/gute...</td>\n",
       "      <td>https://github.com/ricsinaruto/gutenberg-dialog</td>\n",
       "      <td>https://www.dropbox.com/s/jz15wcsceacaqva/gute...</td>\n",
       "      <td>Dialogues extracted from the Project Gutenberg...</td>\n",
       "      <td>Text</td>\n",
       "      <td>14773741</td>\n",
       "      <td>Utterance</td>\n",
       "      <td>327519461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Reddit Corpus</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>https://www.dropbox.com/s/k7cun7f6x2guwva/redd...</td>\n",
       "      <td>https://convokit.cornell.edu/documentation/sub...</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/p999uknmzhx6f49...</td>\n",
       "      <td>A collection of Corpuses of Reddit data built ...</td>\n",
       "      <td>Text</td>\n",
       "      <td>74468</td>\n",
       "      <td>Utterance</td>\n",
       "      <td>3080662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Dataset   Short name   \n",
       "0        Internet Movie Script Database        IMSDb  \\\n",
       "1               Movie Dialogues Dataset       Movies   \n",
       "2         Switchboard Dialog Act Corpus  Switchboard   \n",
       "3                  Supreme Court Corpus       SCOTUS   \n",
       "4                     Tennis Interviews       Tennis   \n",
       "5            Persuasion for Good Corpus          PfG   \n",
       "6   Intelligence Squared Debates Corpus          IQ2   \n",
       "7   Group Affect and Performance Corpus          GAP   \n",
       "8                             The Chair        Chair   \n",
       "9                        Friends Corpus      Friends   \n",
       "10           Gutenberg Dialogue Dataset    Gutenberg   \n",
       "11                        Reddit Corpus       Reddit   \n",
       "\n",
       "                                             Data URL   \n",
       "0   https://www.dropbox.com/scl/fi/ct39vqqq9sjqyyh...  \\\n",
       "1   https://www.dropbox.com/s/881yuhil48v6q1n/movi...   \n",
       "2   https://www.dropbox.com/s/qvx4211u41l2ex4/swit...   \n",
       "3   https://www.dropbox.com/s/icxk3ubo2u2brzq/supr...   \n",
       "4   https://www.dropbox.com/s/q7bfirllnu32mao/tenn...   \n",
       "5   https://www.dropbox.com/scl/fi/ei7uxv9husg9noj...   \n",
       "6   https://www.dropbox.com/scl/fi/srg1j0m4rhgoqhl...   \n",
       "7   https://www.dropbox.com/scl/fi/j1zh1pey7m8kcyr...   \n",
       "8   https://www.dropbox.com/scl/fi/9cpj3t1n1ktxghu...   \n",
       "9   https://www.dropbox.com/s/nfaa6ap0ws1rqjy/frie...   \n",
       "10  https://www.dropbox.com/s/84rid3cboynutmr/gute...   \n",
       "11  https://www.dropbox.com/s/k7cun7f6x2guwva/redd...   \n",
       "\n",
       "                                           Source URL   \n",
       "0                                   https://imsdb.com  \\\n",
       "1   https://convokit.cornell.edu/documentation/mov...   \n",
       "2   https://convokit.cornell.edu/documentation/swi...   \n",
       "3   https://convokit.cornell.edu/documentation/sup...   \n",
       "4   https://convokit.cornell.edu/documentation/ten...   \n",
       "5   https://convokit.cornell.edu/documentation/per...   \n",
       "6   https://convokit.cornell.edu/documentation/iq2...   \n",
       "7   https://convokit.cornell.edu/documentation/gap...   \n",
       "8          https://scrapsfromtheloft.com/?s=THE+CHAIR   \n",
       "9   https://convokit.cornell.edu/documentation/fri...   \n",
       "10    https://github.com/ricsinaruto/gutenberg-dialog   \n",
       "11  https://convokit.cornell.edu/documentation/sub...   \n",
       "\n",
       "                                          Results URL   \n",
       "0   https://www.dropbox.com/scl/fi/3gq5ieq7l25719i...  \\\n",
       "1   https://www.dropbox.com/scl/fi/arxkyhub2fi6qh5...   \n",
       "2   https://www.dropbox.com/scl/fi/1o7wqdlc1oo26y6...   \n",
       "3   https://www.dropbox.com/scl/fi/zxkvlrg4lfxcv7c...   \n",
       "4   https://www.dropbox.com/scl/fi/d3g83mtz4mqhbpm...   \n",
       "5   https://www.dropbox.com/scl/fi/zmumd8uno58cqzo...   \n",
       "6   https://www.dropbox.com/scl/fi/3d4eha6r6xop7h0...   \n",
       "7   https://www.dropbox.com/scl/fi/prk03sodn4pg895...   \n",
       "8   https://www.dropbox.com/scl/fi/to0642t939pvrtz...   \n",
       "9   https://www.dropbox.com/scl/fi/mkxc114g90rifsm...   \n",
       "10  https://www.dropbox.com/s/jz15wcsceacaqva/gute...   \n",
       "11  https://www.dropbox.com/scl/fi/p999uknmzhx6f49...   \n",
       "\n",
       "                                          Description    Category   \n",
       "0   A collection of transcripts from roughly 1000 ...        Film  \\\n",
       "1   A large collection of fictional conversations ...        Film   \n",
       "2   A collection of five-minute telephone conversa...      Speech   \n",
       "3   A collection of cases from the U.S. Supreme Co...      Speech   \n",
       "4   Transcripts for tennis singles post-match pres...      Speech   \n",
       "5   A collection of online conversations generated...      Speech   \n",
       "6   This dataset contains transcripts of debates h...      Speech   \n",
       "7   Group members completed a Winter Survival Task...      Speech   \n",
       "8       Scraped transcripts from The Chair, Season 1.  Television   \n",
       "9   A collection of all the conversations that occ...  Television   \n",
       "10  Dialogues extracted from the Project Gutenberg...        Text   \n",
       "11  A collection of Corpuses of Reddit data built ...        Text   \n",
       "\n",
       "    Number of observations Observation type  Number of words  \n",
       "0                     1091       Transcript         26023348  \n",
       "1                   304713        Utterance          3209921  \n",
       "2                   122646        Utterance          2052779  \n",
       "3                  1700789        Utterance         71889094  \n",
       "4                   163948        Utterance          7043118  \n",
       "5                    20932        Utterance           351759  \n",
       "6                    26562        Utterance          1898509  \n",
       "7                     8009        Utterance            45989  \n",
       "8                        6       Transcript            19197  \n",
       "9                    67373        Utterance           622894  \n",
       "10                14773741        Utterance        327519461  \n",
       "11                   74468        Utterance          3080662  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = Path.cwd().parent.joinpath('data', 'metaanalysis-datasets.xlsx')\n",
    "data = pd.read_excel(data_list)\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect past and future events\n",
    "\n",
    "For each dataset in the `data` DataFrame, we'll:\n",
    "  - Download and extract the dataset if it doesn't already exist locally\n",
    "  - Check to see whether the metaanalysis has already been run on that folder.  If not, we'll run the `process_folder` function on the dataset's directory and save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_name(url):\n",
    "    return [s for s in url.split('/') if '.zip' in s][0].split('?')[0][:-4]\n",
    "\n",
    "def download_dataset(url, outdir):\n",
    "    # Download dataset\n",
    "    filename = get_folder_name(url) + '.zip'\n",
    "    x = requests.get(url)\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(x.content)\n",
    "    \n",
    "    # Unzip dataset\n",
    "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(outdir)\n",
    "\n",
    "    # Delete zip file\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: Internet Movie Script Database\n",
      "Processing dataset: Movie Dialogues Dataset\n",
      "Processing dataset: Switchboard Dialog Act Corpus\n",
      "Processing dataset: Supreme Court Corpus\n",
      "Processing dataset: Tennis Interviews\n",
      "Processing dataset: Persuasion for Good Corpus\n",
      "Processing dataset: Intelligence Squared Debates Corpus\n",
      "Processing dataset: Group Affect and Performance Corpus\n",
      "Processing dataset: The Chair\n",
      "Processing dataset: Friends Corpus\n",
      "Processing dataset: Gutenberg Dialogue Dataset\n",
      "Processing dataset: Reddit Corpus\n"
     ]
    }
   ],
   "source": [
    "datadir = Path.cwd().parent.joinpath('data')\n",
    "results = []\n",
    "sentence = []\n",
    "\n",
    "# should we just download the already-completed results or compute them from scratch?\n",
    "force_rerun = False\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "# for i, row in data[9:10].iterrows():\n",
    "    print('Processing dataset: ' + row['Dataset'])\n",
    "    results_fname = datadir.joinpath(row['Short name'].lower() + '_results.pkl')\n",
    "    if not results_fname.exists():\n",
    "        if force_rerun:\n",
    "            # check whether the dataset exists locally and has at least 5 .txt files\n",
    "            next_datadir = datadir.joinpath(get_folder_name(row['Data URL']))\n",
    "            if not (next_datadir.exists() and len(lsdir(str(next_datadir.joinpath('*.txt')))) >= 5):\n",
    "                # download the dataset\n",
    "                download_dataset(row['Data URL'], datadir)\n",
    "            \n",
    "            # process the dataset\n",
    "            df_results, sentence_dfs = process_folder(next_datadir, past_keywords, future_keywords)\n",
    "            with open(results_fname, 'wb') as f:\n",
    "                pickle.dump([df_results, sentence_dfs], f)\n",
    "        else:\n",
    "            x = requests.get(row['Results URL'])\n",
    "            with open(results_fname, 'wb') as f:\n",
    "                f.write(x.content)\n",
    "    \n",
    "    with open(results_fname, 'rb') as f:\n",
    "        next_results, sentence_dfs = pickle.load(f)\n",
    "    \n",
    "    next_results = next_results.reset_index().rename(columns={\"index\": \"filename\"}).melt(id_vars=[\"filename\"],          \n",
    "                                        var_name=\"tense\", value_name=\"count\")\n",
    "    next_results['proportion'] = next_results['count'] / next_results.groupby('filename')['count'].transform('sum')\n",
    "    next_results['Dataset'] = row['Short name']\n",
    "\n",
    "    results.append(next_results)\n",
    "    sentence.append(sentence_dfs)\n",
    "results = pd.concat(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the Chair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "d = 0\n",
    "\n",
    "for key in sentence[d].keys():\n",
    "    res = {}\n",
    "    # res['dataset'] = data['Short name'][d]\n",
    "    # res['n_files'] = len(sentence[d])\n",
    "    res['episode'] = key\n",
    "    res['n_sentences'] = len(sentence[d][key])\n",
    "    res['n_past_refs_c'] = sentence[d][key]['past'].astype(bool).sum()\n",
    "    res['n_future_refs_c'] = sentence[d][key]['future'].astype(bool).sum()\n",
    "    print(res)\n",
    "    results.append(res)\n",
    "    \n",
    "# pd.DataFrame(results).to_csv(\"../data/the_chair/the_chair_auto_reference_counts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for d in range(len(sentence)):\n",
    "    res = {}\n",
    "    res['dataset'] = data_filter['Short name'][d]\n",
    "    res['n_files'] = len(sentence[d])\n",
    "    res['n_sentences'] = sum([len(sentence[d][key]) for key in sentence[d].keys()])\n",
    "    res['n_past_refs_c'] = sum([sentence[d][key]['past'].astype(bool).sum() for key in sentence[d].keys()])\n",
    "    res['n_future_refs_c'] = sum([sentence[d][key]['future'].astype(bool).sum() for key in sentence[d].keys()])\n",
    "    results.append(res)\n",
    "    \n",
    "# pd.DataFrame(results).to_csv(\"../data/ref_counts_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figure S12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.palettes import Category20c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = pd.read_csv(\"../data/the_chair/the_chair_auto_reference_counts.csv\")\n",
    "manual = pd.read_csv(\"../data/the_chair/the_chair_manual_reference_counts.csv\")\n",
    "\n",
    "auto['auto_ratio'] = auto['Past']/auto['Future']\n",
    "manual['manual_ratio'] = manual['Past']/manual['Future']\n",
    "auto_long = auto[['Episode','Past','Future']].melt(var_name='Direction', value_name='auto_count', id_vars=['Episode'])\n",
    "manual_long = manual[['Episode','Past','Future']].melt(var_name='Direction', value_name='manual_count', id_vars=['Episode'])\n",
    "auto_long['auto_proportion'] = auto_long['auto_count'] / auto_long.groupby('Episode')['auto_count'].transform('sum')\n",
    "manual_long['manual_proportion'] = manual_long['manual_count'] / manual_long.groupby('Episode')['manual_count'].transform('sum')\n",
    "\n",
    "count_all_long = manual_long.merge(auto_long, on=['Episode','Direction'])\n",
    "count_all = manual.merge(auto, on=['Episode'])\n",
    "count_all['manual_prop'] = count_all['Past_x']/(count_all['Past_x']+count_all['Future_x'])\n",
    "count_all['auto_prop'] = count_all['Past_y']/(count_all['Past_y']+count_all['Future_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-5e7cece96c7c4cf4936664c742b075b6\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-5e7cece96c7c4cf4936664c742b075b6\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-5e7cece96c7c4cf4936664c742b075b6\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 14, \"labelFontWeight\": \"normal\", \"titleFontSize\": 14, \"titleFontWeight\": \"normal\"}, \"concat\": {\"spacing\": 50}, \"legend\": {}, \"title\": {\"anchor\": \"start\", \"fontSize\": 20}}, \"hconcat\": [{\"data\": {\"name\": \"data-8f38ffeb03340ce2de25c70f02a2cafa\"}, \"facet\": {\"column\": {\"field\": \"Direction\", \"header\": {\"labelFontSize\": 14, \"labelOrient\": \"top\", \"titleFontSize\": 14, \"titleFontWeight\": \"normal\", \"titlePadding\": 0}, \"sort\": \"descending\", \"title\": \"\", \"type\": \"nominal\"}}, \"spec\": {\"layer\": [{\"mark\": {\"type\": \"bar\", \"color\": \"#bdbdbd\"}, \"encoding\": {\"x\": {\"field\": \"Episode\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"auto_count\", \"title\": \"Number of references\", \"type\": \"quantitative\"}}, \"width\": {\"step\": 30}}, {\"mark\": {\"type\": \"tick\", \"color\": \"#8C6238\", \"thickness\": 2}, \"encoding\": {\"x\": {\"field\": \"Episode\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"manual_count\", \"type\": \"quantitative\"}}}]}, \"title\": \"A\"}, {\"layer\": [{\"mark\": {\"type\": \"bar\", \"color\": \"#bdbdbd\"}, \"encoding\": {\"x\": {\"field\": \"Episode\", \"type\": \"ordinal\"}, \"y\": {\"axis\": {\"format\": \"%\"}, \"field\": \"auto_prop\", \"scale\": {\"domain\": [0, 1]}, \"title\": \"Past / (Past + Future) %\", \"type\": \"quantitative\"}}, \"width\": {\"step\": 30}}, {\"mark\": {\"type\": \"tick\", \"color\": \"#8C6238\", \"thickness\": 2}, \"encoding\": {\"x\": {\"field\": \"Episode\", \"type\": \"ordinal\"}, \"y\": {\"axis\": {\"format\": \"%\"}, \"field\": \"manual_prop\", \"scale\": {\"domain\": [0, 1]}, \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-723fe09044526ebf24326bdcf66425ba\"}, \"title\": \"B\"}, {\"layer\": [{\"mark\": {\"type\": \"bar\", \"color\": \"#bdbdbd\"}, \"encoding\": {\"x\": {\"field\": \"Episode\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"auto_ratio\", \"scale\": {\"base\": 2, \"domain\": [1, 6], \"type\": \"log\"}, \"title\": \"Past / Future Ratio (log scale)\", \"type\": \"quantitative\"}}, \"width\": {\"step\": 30}}, {\"mark\": {\"type\": \"tick\", \"color\": \"#8C6238\", \"thickness\": 2}, \"encoding\": {\"x\": {\"field\": \"Episode\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"manual_ratio\", \"scale\": {\"base\": 2, \"domain\": [1, 6], \"type\": \"log\"}, \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-723fe09044526ebf24326bdcf66425ba\"}, \"title\": \"C\"}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-8f38ffeb03340ce2de25c70f02a2cafa\": [{\"Episode\": 1, \"Direction\": \"Past\", \"manual_count\": 60, \"manual_proportion\": 0.759493670886076, \"auto_count\": 112, \"auto_proportion\": 0.6021505376344086}, {\"Episode\": 2, \"Direction\": \"Past\", \"manual_count\": 29, \"manual_proportion\": 0.6744186046511628, \"auto_count\": 80, \"auto_proportion\": 0.5555555555555556}, {\"Episode\": 3, \"Direction\": \"Past\", \"manual_count\": 43, \"manual_proportion\": 0.5657894736842105, \"auto_count\": 123, \"auto_proportion\": 0.6}, {\"Episode\": 4, \"Direction\": \"Past\", \"manual_count\": 30, \"manual_proportion\": 0.6, \"auto_count\": 108, \"auto_proportion\": 0.5901639344262295}, {\"Episode\": 5, \"Direction\": \"Past\", \"manual_count\": 37, \"manual_proportion\": 0.7872340425531915, \"auto_count\": 116, \"auto_proportion\": 0.6373626373626373}, {\"Episode\": 6, \"Direction\": \"Past\", \"manual_count\": 27, \"manual_proportion\": 0.6923076923076923, \"auto_count\": 121, \"auto_proportion\": 0.55}, {\"Episode\": 1, \"Direction\": \"Future\", \"manual_count\": 19, \"manual_proportion\": 0.24050632911392406, \"auto_count\": 74, \"auto_proportion\": 0.3978494623655914}, {\"Episode\": 2, \"Direction\": \"Future\", \"manual_count\": 14, \"manual_proportion\": 0.32558139534883723, \"auto_count\": 64, \"auto_proportion\": 0.4444444444444444}, {\"Episode\": 3, \"Direction\": \"Future\", \"manual_count\": 33, \"manual_proportion\": 0.4342105263157895, \"auto_count\": 82, \"auto_proportion\": 0.4}, {\"Episode\": 4, \"Direction\": \"Future\", \"manual_count\": 20, \"manual_proportion\": 0.4, \"auto_count\": 75, \"auto_proportion\": 0.4098360655737705}, {\"Episode\": 5, \"Direction\": \"Future\", \"manual_count\": 10, \"manual_proportion\": 0.2127659574468085, \"auto_count\": 66, \"auto_proportion\": 0.3626373626373626}, {\"Episode\": 6, \"Direction\": \"Future\", \"manual_count\": 12, \"manual_proportion\": 0.3076923076923077, \"auto_count\": 99, \"auto_proportion\": 0.45}], \"data-723fe09044526ebf24326bdcf66425ba\": [{\"Episode\": 1, \"Past_x\": 60, \"Future_x\": 19, \"manual_ratio\": 3.1578947368421053, \"Total\": 457, \"Past_y\": 112, \"Future_y\": 74, \"auto_ratio\": 1.5135135135135136, \"manual_prop\": 0.759493670886076, \"auto_prop\": 0.6021505376344086}, {\"Episode\": 2, \"Past_x\": 29, \"Future_x\": 14, \"manual_ratio\": 2.0714285714285716, \"Total\": 501, \"Past_y\": 80, \"Future_y\": 64, \"auto_ratio\": 1.25, \"manual_prop\": 0.6744186046511628, \"auto_prop\": 0.5555555555555556}, {\"Episode\": 3, \"Past_x\": 43, \"Future_x\": 33, \"manual_ratio\": 1.303030303030303, \"Total\": 518, \"Past_y\": 123, \"Future_y\": 82, \"auto_ratio\": 1.5, \"manual_prop\": 0.5657894736842105, \"auto_prop\": 0.6}, {\"Episode\": 4, \"Past_x\": 30, \"Future_x\": 20, \"manual_ratio\": 1.5, \"Total\": 442, \"Past_y\": 108, \"Future_y\": 75, \"auto_ratio\": 1.44, \"manual_prop\": 0.6, \"auto_prop\": 0.5901639344262295}, {\"Episode\": 5, \"Past_x\": 37, \"Future_x\": 10, \"manual_ratio\": 3.7, \"Total\": 508, \"Past_y\": 116, \"Future_y\": 66, \"auto_ratio\": 1.7575757575757576, \"manual_prop\": 0.7872340425531915, \"auto_prop\": 0.6373626373626373}, {\"Episode\": 6, \"Past_x\": 27, \"Future_x\": 12, \"manual_ratio\": 2.25, \"Total\": 474, \"Past_y\": 121, \"Future_y\": 99, \"auto_ratio\": 1.2222222222222223, \"manual_prop\": 0.6923076923076923, \"auto_prop\": 0.55}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bar_count = alt.Chart().mark_bar(color=Category20c[20][18]).encode(\n",
    "    x='Episode:O',\n",
    "    y=alt.Y('auto_count', title=\"Number of references\"),\n",
    "    # column='direction'\n",
    ").properties(\n",
    "    width=alt.Step(30)  # controls width of bar.\n",
    ")\n",
    "tick_count = alt.Chart().mark_tick(color='#8C6238', thickness=2,).encode(\n",
    "    x='Episode:O',\n",
    "    y='manual_count',\n",
    "    # column='Direction',\n",
    ")\n",
    "count_plot = alt.layer(bar, tick, data=count_all_long).facet(column=alt.Column('Direction', sort=\"descending\",  title='', header=alt.Header(labelOrient='top', titleFontSize=14, labelFontSize=14, titleFontWeight='normal', titlePadding=0))).properties(title='A')\n",
    "\n",
    "bar_prop = alt.Chart(count_all).mark_bar(color=Category20c[20][18]).encode(\n",
    "    x='Episode:O',\n",
    "    y=alt.Y('auto_prop', scale=alt.Scale(domain=[0,1]), axis=alt.Axis(format='%'), title=\"Past / (Past + Future) %\"),\n",
    ").properties(\n",
    "    width=alt.Step(30)  # controls width of bar.\n",
    ")\n",
    "\n",
    "tick_prop = alt.Chart(count_all).mark_tick(color='#8C6238', thickness=2,).encode(\n",
    "    x='Episode:O',\n",
    "    y=alt.Y('manual_prop', scale=alt.Scale(domain=[0,1]), axis=alt.Axis(format='%')),\n",
    ")\n",
    "prop_plot = (bar_prop+tick_prop).properties(title='B')\n",
    "bar_ratio = alt.Chart(count_all).mark_bar(color=Category20c[20][18]).encode(\n",
    "    x='Episode:O',\n",
    "    y=alt.Y('auto_ratio', scale=alt.Scale(domain=[1,6], type=\"log\", base=2), title=\"Past / Future Ratio (log scale)\"),\n",
    ").properties(\n",
    "    width=alt.Step(30)  # controls width of bar.\n",
    ")\n",
    "\n",
    "tick_ratio = alt.Chart(count_all).mark_tick(color='#8C6238', thickness=2,).encode(\n",
    "    x='Episode:O',\n",
    "    y=alt.Y('manual_ratio', scale=alt.Scale(domain=[1,6], type=\"log\", base=2)),\n",
    ")\n",
    "\n",
    "ratio_plot = (bar_ratio+tick_ratio).properties(title='C')\n",
    "\n",
    "(count_plot | prop_plot | ratio_plot\n",
    ").configure_legend(\n",
    "\n",
    ").configure_axis(\n",
    "    titleFontSize=14,\n",
    "    labelFontSize=14,\n",
    "    titleFontWeight='normal',\n",
    "    labelFontWeight='normal',\n",
    ").configure_concat(\n",
    "    spacing=50\n",
    ").configure_title(\n",
    "    fontSize=20,\n",
    "    anchor='start',\n",
    "#     offset=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>type</th>\n",
       "      <th>source</th>\n",
       "      <th>full</th>\n",
       "      <th>non-empty</th>\n",
       "      <th>is_equal</th>\n",
       "      <th>past</th>\n",
       "      <th>future</th>\n",
       "      <th>total</th>\n",
       "      <th>corrected_past</th>\n",
       "      <th>corrected_future</th>\n",
       "      <th>past_prop</th>\n",
       "      <th>future_prop</th>\n",
       "      <th>RR</th>\n",
       "      <th>non_past</th>\n",
       "      <th>non_future</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMSDb</td>\n",
       "      <td>Scripted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1091</td>\n",
       "      <td>1091</td>\n",
       "      <td>True</td>\n",
       "      <td>833026</td>\n",
       "      <td>472519</td>\n",
       "      <td>3080674</td>\n",
       "      <td>657475</td>\n",
       "      <td>316525</td>\n",
       "      <td>0.213419</td>\n",
       "      <td>0.102745</td>\n",
       "      <td>2.077166</td>\n",
       "      <td>2423199</td>\n",
       "      <td>2764149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Movies</td>\n",
       "      <td>Scripted</td>\n",
       "      <td>ConvoKit</td>\n",
       "      <td>304713</td>\n",
       "      <td>304446</td>\n",
       "      <td>False</td>\n",
       "      <td>179729</td>\n",
       "      <td>129622</td>\n",
       "      <td>516163</td>\n",
       "      <td>127744</td>\n",
       "      <td>85937</td>\n",
       "      <td>0.247488</td>\n",
       "      <td>0.166492</td>\n",
       "      <td>1.486484</td>\n",
       "      <td>388419</td>\n",
       "      <td>430226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Switchboard</td>\n",
       "      <td>Spontaneous</td>\n",
       "      <td>ConvoKit</td>\n",
       "      <td>122646</td>\n",
       "      <td>122646</td>\n",
       "      <td>True</td>\n",
       "      <td>62464</td>\n",
       "      <td>32372</td>\n",
       "      <td>245461</td>\n",
       "      <td>41488</td>\n",
       "      <td>22079</td>\n",
       "      <td>0.169021</td>\n",
       "      <td>0.089949</td>\n",
       "      <td>1.879071</td>\n",
       "      <td>203973</td>\n",
       "      <td>223382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SCOTUS</td>\n",
       "      <td>Constrained</td>\n",
       "      <td>ConvoKit</td>\n",
       "      <td>1700789</td>\n",
       "      <td>1700789</td>\n",
       "      <td>True</td>\n",
       "      <td>3089509</td>\n",
       "      <td>1802239</td>\n",
       "      <td>3880259</td>\n",
       "      <td>1963578</td>\n",
       "      <td>1207377</td>\n",
       "      <td>0.506043</td>\n",
       "      <td>0.311159</td>\n",
       "      <td>1.626317</td>\n",
       "      <td>1916681</td>\n",
       "      <td>2672882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tennis</td>\n",
       "      <td>Constrained</td>\n",
       "      <td>ConvoKit</td>\n",
       "      <td>163948</td>\n",
       "      <td>163948</td>\n",
       "      <td>True</td>\n",
       "      <td>448444</td>\n",
       "      <td>193802</td>\n",
       "      <td>599172</td>\n",
       "      <td>281669</td>\n",
       "      <td>134638</td>\n",
       "      <td>0.470097</td>\n",
       "      <td>0.224707</td>\n",
       "      <td>2.092047</td>\n",
       "      <td>317503</td>\n",
       "      <td>464534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PfG</td>\n",
       "      <td>Constrained</td>\n",
       "      <td>ConvoKit</td>\n",
       "      <td>20932</td>\n",
       "      <td>20932</td>\n",
       "      <td>True</td>\n",
       "      <td>9695</td>\n",
       "      <td>15520</td>\n",
       "      <td>37184</td>\n",
       "      <td>7408</td>\n",
       "      <td>9771</td>\n",
       "      <td>0.199225</td>\n",
       "      <td>0.262774</td>\n",
       "      <td>0.758162</td>\n",
       "      <td>29776</td>\n",
       "      <td>27413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>IQ2</td>\n",
       "      <td>Constrained</td>\n",
       "      <td>ConvoKit</td>\n",
       "      <td>26562</td>\n",
       "      <td>26317</td>\n",
       "      <td>False</td>\n",
       "      <td>67626</td>\n",
       "      <td>51780</td>\n",
       "      <td>122925</td>\n",
       "      <td>46630</td>\n",
       "      <td>34811</td>\n",
       "      <td>0.379337</td>\n",
       "      <td>0.283189</td>\n",
       "      <td>1.339519</td>\n",
       "      <td>76295</td>\n",
       "      <td>88114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GAP</td>\n",
       "      <td>Constrained</td>\n",
       "      <td>ConvoKit</td>\n",
       "      <td>8009</td>\n",
       "      <td>8009</td>\n",
       "      <td>True</td>\n",
       "      <td>2739</td>\n",
       "      <td>1958</td>\n",
       "      <td>8009</td>\n",
       "      <td>1800</td>\n",
       "      <td>1338</td>\n",
       "      <td>0.224747</td>\n",
       "      <td>0.167062</td>\n",
       "      <td>1.345291</td>\n",
       "      <td>6209</td>\n",
       "      <td>6671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chair</td>\n",
       "      <td>Scripted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>909</td>\n",
       "      <td>663</td>\n",
       "      <td>2900</td>\n",
       "      <td>660</td>\n",
       "      <td>460</td>\n",
       "      <td>0.227586</td>\n",
       "      <td>0.158621</td>\n",
       "      <td>1.434783</td>\n",
       "      <td>2240</td>\n",
       "      <td>2440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Friends</td>\n",
       "      <td>Scripted</td>\n",
       "      <td>ConvoKit</td>\n",
       "      <td>67373</td>\n",
       "      <td>61310</td>\n",
       "      <td>False</td>\n",
       "      <td>32105</td>\n",
       "      <td>23931</td>\n",
       "      <td>107082</td>\n",
       "      <td>22067</td>\n",
       "      <td>16356</td>\n",
       "      <td>0.206076</td>\n",
       "      <td>0.152743</td>\n",
       "      <td>1.349169</td>\n",
       "      <td>85015</td>\n",
       "      <td>90726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gutenberg</td>\n",
       "      <td>Scripted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14773741</td>\n",
       "      <td>14773741</td>\n",
       "      <td>True</td>\n",
       "      <td>14617983</td>\n",
       "      <td>13714226</td>\n",
       "      <td>29119393</td>\n",
       "      <td>10234952</td>\n",
       "      <td>8672030</td>\n",
       "      <td>0.351482</td>\n",
       "      <td>0.297809</td>\n",
       "      <td>1.180226</td>\n",
       "      <td>18884441</td>\n",
       "      <td>20447363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Reddit</td>\n",
       "      <td>Constrained</td>\n",
       "      <td>ConvoKit</td>\n",
       "      <td>74468</td>\n",
       "      <td>72985</td>\n",
       "      <td>False</td>\n",
       "      <td>120512</td>\n",
       "      <td>105127</td>\n",
       "      <td>217924</td>\n",
       "      <td>86513</td>\n",
       "      <td>66700</td>\n",
       "      <td>0.396987</td>\n",
       "      <td>0.306070</td>\n",
       "      <td>1.297046</td>\n",
       "      <td>131411</td>\n",
       "      <td>151224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dataset         type    source      full  non-empty  is_equal  \\\n",
       "0         IMSDb     Scripted       NaN      1091       1091      True   \n",
       "1        Movies     Scripted  ConvoKit    304713     304446     False   \n",
       "2   Switchboard  Spontaneous  ConvoKit    122646     122646      True   \n",
       "3        SCOTUS  Constrained  ConvoKit   1700789    1700789      True   \n",
       "4        Tennis  Constrained  ConvoKit    163948     163948      True   \n",
       "5           PfG  Constrained  ConvoKit     20932      20932      True   \n",
       "6           IQ2  Constrained  ConvoKit     26562      26317     False   \n",
       "7           GAP  Constrained  ConvoKit      8009       8009      True   \n",
       "8         Chair     Scripted       NaN         6          6      True   \n",
       "9       Friends     Scripted  ConvoKit     67373      61310     False   \n",
       "10    Gutenberg     Scripted       NaN  14773741   14773741      True   \n",
       "11       Reddit  Constrained  ConvoKit     74468      72985     False   \n",
       "\n",
       "        past    future     total  corrected_past  corrected_future  past_prop  \\\n",
       "0     833026    472519   3080674          657475            316525   0.213419   \n",
       "1     179729    129622    516163          127744             85937   0.247488   \n",
       "2      62464     32372    245461           41488             22079   0.169021   \n",
       "3    3089509   1802239   3880259         1963578           1207377   0.506043   \n",
       "4     448444    193802    599172          281669            134638   0.470097   \n",
       "5       9695     15520     37184            7408              9771   0.199225   \n",
       "6      67626     51780    122925           46630             34811   0.379337   \n",
       "7       2739      1958      8009            1800              1338   0.224747   \n",
       "8        909       663      2900             660               460   0.227586   \n",
       "9      32105     23931    107082           22067             16356   0.206076   \n",
       "10  14617983  13714226  29119393        10234952           8672030   0.351482   \n",
       "11    120512    105127    217924           86513             66700   0.396987   \n",
       "\n",
       "    future_prop        RR  non_past  non_future  \n",
       "0      0.102745  2.077166   2423199     2764149  \n",
       "1      0.166492  1.486484    388419      430226  \n",
       "2      0.089949  1.879071    203973      223382  \n",
       "3      0.311159  1.626317   1916681     2672882  \n",
       "4      0.224707  2.092047    317503      464534  \n",
       "5      0.262774  0.758162     29776       27413  \n",
       "6      0.283189  1.339519     76295       88114  \n",
       "7      0.167062  1.345291      6209        6671  \n",
       "8      0.158621  1.434783      2240        2440  \n",
       "9      0.152743  1.349169     85015       90726  \n",
       "10     0.297809  1.180226  18884441    20447363  \n",
       "11     0.306070  1.297046    131411      151224  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"ref_counts_summary.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24040006"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total = df['corrected_past'].sum() + df['corrected_future'].sum()\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13471984"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['corrected_past'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5603985290186699"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['corrected_past'].sum() / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10568022"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['corrected_future'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43960147098133"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['corrected_future'].sum() / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMSDb\n",
      "0.0\n",
      "Movies\n",
      "0.0\n",
      "Switchboard\n",
      "0.0\n",
      "SCOTUS\n",
      "0.0\n",
      "Tennis\n",
      "0.0\n",
      "PfG\n",
      "7.651987244822489e-94\n",
      "IQ2\n",
      "0.0\n",
      "GAP\n",
      "4.415379561347529e-20\n",
      "Chair\n",
      "3.6004633077240275e-11\n",
      "Friends\n",
      "7.023120502011673e-227\n",
      "Gutenberg\n",
      "0.0\n",
      "Reddit\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for i in df.iterrows():\n",
    "    print(i[1]['dataset'])\n",
    "    stat, p, dof, expected = chi2_contingency([[i[1]['corrected_past'], i[1]['non_past']], [i[1]['corrected_future'], i[1]['non_future']]])\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
